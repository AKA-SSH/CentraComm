{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c11518-067b-4d71-9d18-525192a3a227",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8540cc-ee53-437f-979c-8a3f0447e4e7",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cfea5aa-0434-42f9-800f-bd22d115f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818233d3-0c10-48c5-aa70-519a0dd82324",
   "metadata": {},
   "source": [
    "## API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2290812c-2e39-4d18-91e8-06175ded3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ffc77-5389-42f8-ae9e-6f512f7624a0",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd53c5d-ea5d-496f-b3d1-f43ff32a5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc96b052-0b08-4b7d-9dba-3eebc65c31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c29320-de6b-4983-b888-5654b9a1a17b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72660a0-ea7f-4cb9-abc8-6952a1f3d89a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edc277b-4ac2-42e5-bb63-7b96790d2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bbdf35c-0932-410c-9023-ccf217f54b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stopword_list]\n",
    "    text_without_stopwords = nlp(' '.join(tokens_without_stopwords))\n",
    "    lemmatized_tokens = [token.lemma_ for token in text_without_stopwords]\n",
    "    cleaned_data = [re.sub(r'[#*]', '', token) for token in lemmatized_tokens]\n",
    "    cleaned_data = [re.sub(r'[-]{2,}', '', token) for token in cleaned_data]\n",
    "    cleaned_data = [token for token in cleaned_data if token.strip()]\n",
    "    cleaned_data = ' '.join(cleaned_data)\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8207f09-1cce-4526-adc3-d55014490e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_character_splitter(text, chunk_size=512, chunk_overlap=64):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, is_separator_regex=False)\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dea83e7-c0aa-4f68-a2bc-880519a421e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_store_and_model(chunks):\n",
    "    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "    library = FAISS.from_documents(chunks, embeddings)\n",
    "    qna_bot = RetrievalQA.from_chain_type(llm=OpenAI(api_key=OPENAI_API_KEY), chain_type='stuff', retriever=library.as_retriever())\n",
    "\n",
    "    return qna_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ea3e50c-7c04-4c3c-b702-17eb9db1c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_the_bot(query, bot):\n",
    "    response = bot.invoke(query)\n",
    "\n",
    "    print(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fbbbf3-a211-4950-9e42-0bc21cb508e8",
   "metadata": {},
   "source": [
    "## Executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4658d4ca-feff-442e-b121-5c8b09fc99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_list = [\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\project\\\\alex_johnson.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\project\\\\emily_chen.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\project\\\\michael_reynolds.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\research\\\\alex_johnson.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\research\\\\jane_smith.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\research\\\\john_doe.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\training\\\\tasktracker.txt',\n",
    "    'C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\training\\\\virtualconnection.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38bf0eca-97d3-46e4-9fbb-5e3ec7fff62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_collection = []\n",
    "\n",
    "for filepath in filepath_list:\n",
    "    raw_text = read_text_file(filepath)\n",
    "    processed_text = preprocess_text(raw_text)\n",
    "    chunks = recursive_character_splitter(processed_text)\n",
    "    chunk_collection.extend(chunks)\n",
    "    \n",
    "qna_bot = embed_store_and_model(chunk_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "167578cc-c5cf-4d88-a6c4-62caf5e983c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The company uses Apache Tika to extract text metadata from PDFs, Elasticsearch for indexing unstructured data, and a relational database for storing structured data. They also plan to incorporate NLP and RAG techniques for their chatbot development and are considering incorporating machine learning techniques for their equipment management project.\n"
     ]
    }
   ],
   "source": [
    "ask_the_bot('What are the technologies used in the company?', qna_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f6af0-b930-44ad-9419-eab27ce29d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
