{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc903eff-2493-4566-8b0a-7eaafc63a9ca",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a0082a-1b53-4a45-990f-3533f53d8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3788a9e3-c47c-473c-a9eb-24db3bf2419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f89c0f-b15d-4636-bf22-7074b5acd233",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39657174-2b84-4637-b2ba-55cd99b32fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf36ce01-f31e-494c-82cb-15c48a301c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d515892-f6f5-461a-9775-552b940470f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d770a5-773c-46d3-925b-3133f6de5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6b5965-ff69-4857-95b8-655496d8128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "797c3159-a5f3-4bb4-b168-2788e0f5d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stopword_list]\n",
    "    text_without_stopwords = nlp(' '.join(tokens_without_stopwords))\n",
    "    lemmatized_tokens = [token.lemma_ for token in text_without_stopwords]\n",
    "    cleaned_data = [re.sub(r'[#*]', '', token) for token in lemmatized_tokens]\n",
    "    cleaned_data = [re.sub(r'[-]{2,}', '', token) for token in cleaned_data]\n",
    "    cleaned_data = [token for token in cleaned_data if token.strip()]\n",
    "    cleaned_data = ' '.join(cleaned_data)\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abd1915a-1b00-4449-bbc4-a3651f0b5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = read_text_file('C:\\\\Users\\\\Akash\\\\Desktop\\\\GenAI\\\\CentraComm\\\\CentraComm\\\\data\\\\project\\\\alex_johnson.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72856715-32f4-4fee-8aad-978f3b2aa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sample_text = preprocess_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44fa9683-e790-4582-b4d0-079bb99c9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d1dc276-8f2e-49cd-a11d-e93ad7e888cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_character_splitter(text, chunk_size=512, chunk_overlap=64):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len, is_separator_regex=False)\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8609f23-6e58-4199-8092-6b8a67466d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = recursive_character_splitter(preprocessed_sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78782cf2-e4d5-41e2-9b92-4f13a3c9a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50036eb8-342a-4a8e-ae52-bbeab0282c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akash\\Desktop\\GenAI\\CentraComm\\CentraComm\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca8c49c7-33a2-448c-b71a-f41e4f593a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8daa4cb8-5997-4ad4-81b2-ab8dc4e704a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass('Enter OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29ce4400-c5db-4783-9a58-3b575d0b633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ff2f8a2-3839-4afd-97da-d2c692222fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "library = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e038b9a3-d1d1-4454-ba8b-4c49715762d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = 'What is the document about?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23d108d9-0ba5-41da-8c04-fe2e337c28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_answer = library.similarity_search(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0b49e5e-e882-4cc7-a7be-ba09d702ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Report title : implement Machine Learning Financial Fraud Detection prepared : Alex Johnson , Senior Data Scientist Date : May 25 , 2024 1 . introduction Financial fraud detection critical maintain integrity financial institution protect customer fraudulent activity . traditional rule - base system often fail adapt evolve fraud tactic , necessitating implementation machine learning ( ML ) technique . project report outline step take implement ML - base fraud detection , methodology use , outcome\n"
     ]
    }
   ],
   "source": [
    "print(query_answer[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca89d5e8-014a-440e-8917-8f0b0cda1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_scores = library.similarity_search_with_score(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f138fc8c-d154-4878-939c-9b2c86c439fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='Project Report title : implement Machine Learning Financial Fraud Detection prepared : Alex Johnson , Senior Data Scientist Date : May 25 , 2024 1 . introduction Financial fraud detection critical maintain integrity financial institution protect customer fraudulent activity . traditional rule - base system often fail adapt evolve fraud tactic , necessitating implementation machine learning ( ML ) technique . project report outline step take implement ML - base fraud detection , methodology use , outcome'),\n",
       " 0.54368895)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1697bad-63c8-4dcd-aa47-f18e5513bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = library.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "244e58e0-bebc-4714-babe-e966bd7bb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(api_key=OPENAI_API_KEY), chain_type='stuff', retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2fb782c-cc0d-4003-90ff-c764425f9b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = 'What are the technologies used in this project?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cd590ca-ee8a-48e8-8b44-3648da095004",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = qa.invoke(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b7201fb-26c9-4492-be70-0274b8a8c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the technologies used in this project?', 'result': ' Machine learning and traditional rule-based systems were used in this project to develop a fraud detection model. Specific technologies mentioned include logistic regression, decision trees, random forests, gradient boosting machines, and neural networks. Additionally, data collection and preprocessing techniques, as well as feature engineering, were utilized in the development of the model.'}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af56873-da0b-46e6-8d62-25eb0320baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
